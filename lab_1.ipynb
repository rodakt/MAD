{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele analizy danych\n",
    "\n",
    "Tomasz Rodak\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Trening w uczeniu maszynowym\n",
    "\n",
    "Trening (uczenie, optymalizacja) w uczeniu maszynowym to proces, podczas którego model wyznacza swoje parametry w oparciu o dostępne dane. Proces ten może być jednorazowy (gdy istnieje wzór na parametry) lub stopniowy, polegający na iteracyjnym dostosowywaniu parametrów. \n",
    "\n",
    "W przypadku iteracyjnym istotą treningu jest ocena modelu. Do oceny wykorzystuje się funkcję zwaną funkcją **straty** lub **kosztu** pokazującą jak bardzo przewidywania modelu odbiegają od rzeczywistych wartości (kóre w uczeniu nadzorowanym są nam znane). Funkcja ta ma różną postać w zależności od tego z jakim problemem mamy do czynienia:\n",
    "* błąd średniokwadratowy (MSE) w przypadku regresji,\n",
    "* entropia krzyżowa w przypadku klasyfikacji,\n",
    "* funkcja wiarygodności w rożnych modelach probabilistycznych.\n",
    "\n",
    "Trening w postaci iteracyjnej wygląda zwykle następująco:\n",
    "1. inicjalizacja modelu z losowymi parametrami,\n",
    "2. obliczenie wartości funkcji straty na zbiorze testowym,\n",
    "3. sprawdzenie, \n",
    "   * czy parametry modelu przestały się istotnie zmieniać (optymalizacja zbieżna), lub\n",
    "   * czy nie przekroczono limitu iteracji (brak zbieżności).\n",
    "   \n",
    "   Jeśli tak, to proces jest przerywany.\n",
    "4. dostosowanie parametrów,\n",
    "5. powrót do punktu 2.\n",
    "\n",
    "Powstaje pytanie w jaki sposób dostosować parametry modelu w punkcie 4? Funkcja straty może być rozpatrywana jako funkcja przekształcająca parametry modelu w wartość oceny na zbiorze testowym. Zatem poszukiwany jest taki układ parametrów, dla którego funkcja straty obliczana na zbiorze testowym osiąga minimum. Dostosowanie parametrów w punkcie 4 może zatem polegać na podmianie bieżącego układu parametrów na taki układ, który wypada bliżej lokalnego minimum funkcji straty. **Algorytm gradientu prostego** (i wielu jego odmian) opiera się na obserwacji, że kierunek przemieszczania się w przestrzeni parametrów w rejon minimum lokalnego wyznacza ujemny **gradient** funkcji straty.\n",
    "\n",
    "Celem tego arkusza jest wizualna demonstracja działania algorytmu gradientu prostego na prostych przykładach funkcji rzeczywistych. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Algorytm gradientu prostego\n",
    "\n",
    "Załóżmy, że mamy daną funkcję rzeczywistą $y=f(x)$, $x\\in\\mathbb{R}^p$. Chcemy znaleźć jej minimum lokalne. Algorytm gradientu prostego wygląda następująco:\n",
    "\n",
    "1. Ustalamy parametry algorytmu:\n",
    "   * $x_0$ - punkt startowy (dowolny),\n",
    "   * $\\eta>0$ - współczynnik uczenia (dowolny, ale nie za duży),\n",
    "2. Obliczamy kolejne punkty $x_1,x_2,\\ldots$ w następujący sposób:\n",
    "   * $x_{n+1} = x_n - \\eta \\cdot \\nabla f(x_n)$, gdzie $\\nabla f(x_n)$ to gradient funkcji $f$ w punkcie $x_n$.\n",
    "3. Proces kończymy, gdy osiągniemy zbieżność, czyli gdy różnice między kolejnymi punktami staną się zaniedbywalnie małe, lub gdy osiągniemy maksymalną liczbę iteracji.\n",
    "4. Zwracamy ostatni punkt $x_n$ jako przybliżenie minimum lokalnego funkcji $f$.\n",
    "\n",
    "Dlaczego to działa? Otóż gradient $f$ w punkcie $x$ wskazuje kierunek, w którym funkcja $f$ **najszybciej rośnie**. Kierunek przeciwny, to kierunek, w którym funkcja $f$ najszybciej maleje. Dlatego przemieszczenie się w kierunku przeciwnym do wskazywanego przez gradient powinno prowadzić do (jakiegoś) minimum lokalnego funkcji $f$ i to jest powód, dla którego do bieżącej wartości $x_n$ dodajemy \n",
    "\n",
    "\\begin{equation*}\n",
    "-\\eta \\cdot f'(x_n)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Przypadek funkcji jednej zmiennej \n",
    "\n",
    "Załóżmy, że $y=f(x)$ jest funkcją jednej zmiennej $x$. W tym przypadku algorytm redukuje się do postaci:\n",
    "\n",
    "1. Ustalamy parametry algorytmu:\n",
    "   * $x_0$ - punkt startowy (dowolny),\n",
    "   * $\\eta>0$ - współczynnik uczenia (dowolny, ale nie za duży),\n",
    "2. Obliczamy kolejne punkty $x_1,x_2,\\ldots$ w następujący sposób:\n",
    "   * $x_{n+1} = x_n - \\eta \\cdot f'(x_n)$, gdzie $f'(x_n)$ to pochodna funkcji $f$ w punkcie $x_n$.\n",
    "3. Proces kończymy, gdy osiągniemy zbieżność, czyli gdy różnice między kolejnymi punktami staną się zaniedbywalnie małe, lub gdy osiągniemy maksymalną liczbę iteracji.\n",
    "4. Zwracamy ostatni punkt $x_n$ jako przybliżenie minimum lokalnego funkcji $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Przykład dla wybranej funkcji jednej zmiennej\n",
    "\n",
    "Na początek wybierz jakąś prostą dobrze znaną funkcję, np. $f(x) = x^2$. Później możesz ją dowolnie zmieniać. \n",
    "\n",
    "Zdefiniuj swoją funkcję w Pythonie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj wykres funkcji na jakimś wybranym przedziale zawierającycm przynajmniej jedno minimum lokalne. Dla funkcji kwadratowej takie minimum jest tylko jedno, ale inne funkcje mogą mieć ich więcej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ustal punkt startowy $x_0$ oraz współczynnik uczenia $\\eta>0$. Wykonaj kilka iteracji algorytmu gradientu prostego i narysuj na wykresie funkcji kolejno uzyskiwane punkty $x_n$. Punkty połącz linią pokazującą ścieżkę optymalizacji. Pochodną obliczaj numerycznie stosując wzór:\n",
    "\n",
    "\\begin{equation*}\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x)}{h},\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie $h$ to liczba bliska zeru (np. $h=10^{-9}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napisz kod, który automatycznie wykona powyższe kroki.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Wnioski\n",
    "\n",
    "Przeprowadź różne eksperymenty zmieniając funkcję, punkt startowy oraz współczynnik uczenia. Odpowiedz na pytania:\n",
    "\n",
    "1. co się dzieje, gdy współczynnik uczenia jest zbyt duży?\n",
    "2. co się dzieje, gdy współczynnik uczenia jest zbyt mały?\n",
    "3. jak ostateczny wynik algorytmu zależy od wyboru punktu startowego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Przypadek funkcji dwóch zmiennych\n",
    "\n",
    "Załóżmy teraz, że $z=f(x,y)$ jest funkcją dwóch zmiennych $x$ i $y$. Oto postać algorytmu dla tego przypadku:\n",
    "\n",
    "1. Ustalamy parametry algorytmu:\n",
    "   * $(x_0, y_0)$ - punkt startowy (dowolny),\n",
    "   * $\\eta>0$ - współczynnik uczenia (dowolny, ale nie za duży),\n",
    "2. Obliczamy kolejne punkty $(x_1,y_1), (x_2,y_2),\\ldots$ w następujący sposób:\n",
    "\n",
    "    \\begin{gather*}\n",
    "    x_{n+1} = x_n - \\eta \\cdot \\frac{\\partial f}{\\partial x}(x_n, y_n) \\\\\n",
    "    y_{n+1} = y_n - \\eta \\cdot \\frac{\\partial f}{\\partial y}(x_n, y_n)\n",
    "    \\end{gather*}\n",
    "\n",
    "3. Proces kończymy, gdy osiągniemy zbieżność, czyli gdy różnice między kolejnymi punktami staną się zaniedbywalnie małe, lub gdy osiągniemy maksymalną liczbę iteracji.\n",
    "4. Zwracamy ostatni punkt $(x_n, y_n)$ jako przybliżenie minimum lokalnego funkcji $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Przykład dla wybranej funkcji dwóch zmiennych\n",
    "\n",
    "Na początek wybierz jakąś prostą dobrze znaną funkcję, np. $f(x,y) = x^2 + y^2$ lub $f(x,y) = (x-1)^2 + 10(y+2)^2$. Możesz też wypróbować coś o bardziej wyszukanym zachowaniu, np. funkcję Rosenbrocka \n",
    "\n",
    "\\begin{equation*}\n",
    "f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "\\end{equation*}\n",
    "\n",
    "czy funkcję Himmelblaua\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Zdefiniuj swoją funkcję w Pythonie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysuj wykres 3D funkcji oraz wykres konturowy (widok z góry) na jakimś wybranym obszarze zawierającym przynajmniej jedno minimum lokalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ustal punkt startowy $(x_0, y_0)$ oraz współczynnik uczenia $\\eta>0$. Wykonaj kilka iteracji algorytmu gradientu prostego i narysuj na mapie konturowej kolejno uzyskiwane punkty $(x_n, y_n)$ połączone linią pokazującą ścieżkę optymalizacji. \n",
    "\n",
    "Pochodne cząstkowe obliczaj numerycznie stosując wzory:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial x}(x,y) \\approx \\frac{f(x+h, y) - f(x, y)}{h}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial y}(x,y) \\approx \\frac{f(x, y+h) - f(x, y)}{h}\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie $h$ to liczba bliska zeru (np. $h=10^{-9}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napisz kod, który automatycznie wykona powyższe kroki z kryterium zatrzymania opartym na zbieżności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Wnioski\n",
    "\n",
    "Przeprowadź różne eksperymenty zmieniając funkcję, punkt startowy oraz współczynnik uczenia. Odpowiedz na pytania:\n",
    "\n",
    "1. Co się dzieje, gdy współczynnik uczenia jest zbyt duży?\n",
    "2. Co się dzieje, gdy współczynnik uczenia jest zbyt mały?\n",
    "3. Jak ostateczny wynik algorytmu zależy od wyboru punktu startowego?\n",
    "4. Jak zachowuje się algorytm dla różnych funkcji (np. funkcja Rosenbrocka vs. prosta parabola)?\n",
    "5. Czy algorytm zawsze znajduje globalne minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
