{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".wzor {\n",
    "  /* width: auto; */\n",
    "  /* padding: auto; */\n",
    "  border: 0.5rem solid gray;\n",
    "  /* margin: 0; */\n",
    "  background-color: lightgray;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele analizy danych\n",
    "\n",
    "### 2024/2025, semestr zimowy\n",
    "Tomasz Rodak\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykład VIII\n",
    "\n",
    "ISLP, rozdział 4.4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literatura\n",
    "   1. James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor,\n",
    "      J. (2023). An Introduction to Statistical Learning: With\n",
    "      Applications in Python.\n",
    "      \n",
    "      [ISLP](https://www.statlearning.com/)\n",
    "   \n",
    "   2. Bishop, C. M., & Nasrabadi, N. M. (2006). Pattern recognition and\n",
    "      machine learning (Vol. 4, No. 4, p. 738). New York: springer. \n",
    "   \n",
    "      [PRML](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)\n",
    "   \n",
    "   3. Kroese, D. P., Botev, Z., Taimre, T., & Vaisman, R. (2019). Data\n",
    "      science and machine learning: mathematical and statistical\n",
    "      methods. CRC Press.\n",
    "   \n",
    "      [https://people.smp.uq.edu.au/DirkKroese/DSML/](https://people.smp.uq.edu.au/DirkKroese/DSML/)\n",
    "   \n",
    "   4. Hastie, T., Tibshirani, R., Friedman, J. H., & Friedman,\n",
    "      J. H. (2009). The elements of statistical learning: data mining,\n",
    "      inference, and prediction (Vol. 2, pp. 1-758). New York:\n",
    "      springer.\n",
    "   \n",
    "      [https://hastie.su.domains/ElemStatLearn/](https://hastie.su.domains/ElemStatLearn/)\n",
    "   \n",
    "   5. Murphy, K. P. (2022). Probabilistic machine learning: an\n",
    "      introduction. MIT press.\n",
    "   \n",
    "      [https://probml.github.io/pml-book/book1.html](https://probml.github.io/pml-book/book1.html)\n",
    "   \n",
    "   6. Murphy, K. P. (2023). Probabilistic machine learning: Advanced\n",
    "      topics. MIT press.\n",
    "   \n",
    "      [https://probml.github.io/pml-book/book2.html](https://probml.github.io/pml-book/book2.html)\n",
    "   \n",
    "   7. Bishop, C. M., & Bishop, H (2024). Deep learning. Springer.\n",
    "   \n",
    "      [https://www.bishopbook.com/](https://www.bishopbook.com/)\n",
    "   \n",
    "   8. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.\n",
    "   \n",
    "      [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis as LDA,\n",
    "    QuadraticDiscriminantAnalysis as QDA\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele generatywne w problemach klasyfikacji: QDA, Naive Bayes\n",
    "\n",
    "Przpomnijmy wprowadzony w poprzednim wykładzie schemat modelu generatywnego w problemie klasyfikacji. Niech $X$ będzie wektorem losowym cech, a $Y\\in\\{1,\\ldots,K\\}$ zmienną losową oznaczającą klasę. Na podstawie danych modelujemy rozkład a priori $P(Y=k)$ występowania oraz rozkłady warunkowe $p(x|Y=k)$ cech dla każdej klasy $k$ z osobna. Gdy te rozkłady już mamy, to możemy obliczyć rozkład a posteriori $P(Y=k|X=x)$ ze wzoru Bayesa:\n",
    "\n",
    "$$\n",
    "P(Y=k|x) = \\frac{P(Y=k)p(x|Y=k)}{\\sum_{k=1}^K P(Y=k)p(x|Y=k)}.\n",
    "$$\n",
    "\n",
    "Tak utworzony model pozwala na zdefiniowanie dyskryminatora w postaci klasyfikatora Bayesa: obserwacji $x$ przypisujemy tę klasę $k$, dla której $P(Y=k|x)$ jest największe oraz na generowanie nowych obserwacji $(x,y)$ zgodnie z rozkładem łącznym $p(x, y) = p(x|Y=k)P(Y=k)$.\n",
    "\n",
    "Rozkłady a priori $P(Y=k)$ estymujemy na podstawie częstości występowania klas w zbiorze uczącym. Estymacja rozkładów warunkowych $p(x|Y=k)$ jest zadaniem trudniejszym i wymaga zastosowania dodatkowych założeń. W omówionym w poprzednim wykładzie modelu LDA zakładaliśmy, że rozkłady warunkowe $p(x|Y=k)$ są rozkładami normalnymi o jednakowych macierzach kowariancji. Mamy zatem założenie o postaci rozkładów (są to rozkłady normalne) i założenie o parametrach (macierz kowariancji jest taka sama dla wszystkich klas). W tym wykładzie omówimy kolejne dwa modele generatywne, QDA i Naive Bayes, które mają inne założenia o postaci rozkładów warunkowych $p(x|Y=k)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "W modelu QDA, podobnie jak w LDA, zakładamy, że rozkłady warunkowe $p(x|Y=k)$ są rozkładami normalnymi. Jednak w przeciwieństwie do LDA, nie zakładamy, że macierze kowariancji są jednakowe dla wszystkich klas. Mamy zatem założenie o postaci rozkładów (są to rozkłady normalne) i założenie o parametrach (macierz kowariancji może być inna dla każdej klasy). W modelu QDA rozkład warunkowy $p(x|Y=k)$ jest rozkładem normalnym o gęstości\n",
    "\n",
    "$$\n",
    "p(x|Y=k) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\right),\n",
    "$$\n",
    "\n",
    "gdzie $\\mu_k$ jest wektorem średnich cech dla klasy $k$, a $\\Sigma_k$ jest macierzą kowariancji cech dla klasy $k$. Rozkład a priori $P(Y=k)$ estymujemy na podstawie częstości występowania klas w zbiorze uczącym. \n",
    "\n",
    "Licząc analogicznie jak w przypadku modelu LDA, otrzymujemy funkcje dyskryminacyjne postaci\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) - \\frac{1}{2}\\log|\\Sigma_k| + \\log P(Y=k).\n",
    "$$\n",
    "\n",
    "Klasyfikator Bayesa przypisuje obserwacji $x$ tę klasę $k$, dla której $\\delta_k(x)$ jest największe. Funkcje są bardziej skomplikowane niż w przypadku LDA, ponieważ macierze kowariancji $\\Sigma_k$ zależą od klasy $k$ i nie skracają się podczas upraszczania wyrażeń. Ponadto stopień tych funkcji względem $x$ jest dwa (w LDA był jeden) i dlatego model ten nazywamy *Quadratic Discriminant Analysis*. W konsekwencji granice decyzyjne w modelu QDA są zbudowane z fragmentów hiperpowierzchni stopnia drugiego (w zależnosci od wymiaru $p$ są to parabole, hiperbole, elipsy, paraboloidy, hiperboloidy, elipsoidy, itd.). \n",
    "\n",
    "Szacowanie parametrów modelu QDA jest zadaniem obliczeniowo trudniejszym niż w przypadku LDA, ponieważ musimy estymować $K$ macierzy kowariancji $\\Sigma_k$ zamiast jednej macierzy kowariancji $\\Sigma$ w przypadku LDA. Oznacza to konieczność obliczenia znacznie większej liczby parametrów. Z drugiej strony, model QDA jest mniej restrykcyjny niż LDA. Jeśli spodziewamy się nieliniowej granicy decyzyjnej, to model QDA może być lepszym wyborem niż LDA, gdyż da wtedy mniejszy *bias* (błąd obciążenia) niż LDA. LDA może w tym przypadku skutkować modelem niedouczonym. Jeśli jednak spodziewamy się, że granica decyzyjna jest liniowa, to model QDA będzie miał większy *variance* (błąd wariancji) niż LDA, ponieważ będzie miał więcej parametrów do dopasowania. W tym przypadku model QDA może skutkować modelem przeuczonym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naiwny klasyfikator Bayesa (*Naive Bayes*)\n",
    "\n",
    "W modela LDA i QDA zakładaliśmy, że rozkłady warunkowe $p(x|Y=k)$ są rozkładami normalnymi. W obu tych przypadkach szukaliśmy, przy założeniu, że znana jest klasa $k$, rozkładów *łącznych* wektorów losowych $X$. Oznacza to, że szukając rozkładu $X|Y=k$ estymujemy nie tylko rozkłady poszczególnych składowych wektora cech $X_j|Y=k$, ale także kowariancje między nimi. Problem ten ulega znacznemu uproszczeniu w naiwnym klasyfikatorze Bayesa (*Naive Bayes*), w którym zakładamy, że cechy $X_j$ są niezależne, gdy znamy klasę $k$. Wówczas warunkowy rozkład łączny $X|Y=k$ jest iloczynem rozkładów warunkowych poszczególnych składowych $X_j|Y=k$:\n",
    "\n",
    "$$\n",
    "p(x|Y=k) = \\prod_{j=1}^p p(x_j|Y=k).\n",
    "$$\n",
    "\n",
    "Założenie o niezależności cech jest bardzo silne i rzadko jest spełnione w rzeczywistych problemach. Jednak w wielu przypadkach, nawet gdy to założenie nie jest spełnione naiwny klasyfikator Bayesa daje dobre wyniki. \n",
    "\n",
    "Rozkład a posteriori ma wówczas postać:\n",
    "\n",
    "$$\n",
    "P(Y=k|x) = \\frac{P(Y=k)\\prod_{j=1}^p p(x_j|Y=k)}{\\sum_{k=1}^K P(Y=k)\\prod_{j=1}^p p(x_j|Y=k)}.\n",
    "$$\n",
    "\n",
    "Aby zakończyć budowę modelu musimy zdefiniować rozkłady warunkowe $p(x_j|Y=k)$ dla każdej cechy $X_j$ i każdej klasy $k$. W tym celu możemy wykorzystać dowolny rozkład prawdopodobieństwa. Najczęściej stosuje się rozkłady normalne, Bernoulliego lub wielomianowe. \n",
    "\n",
    "* Jeśli cecha $X_j$ jest zmienną losową ciągłą, to możemy przyjąć, że $(X_j|Y=k)\\sim N(\\mu_{jk}, \\sigma_{jk}^2)$, gdzie $\\mu_{jk}$ i $\\sigma_{jk}^2$ są parametrami rozkładu normalnego dla cechy $X_j$ i klasy $k$. Wówczas rozkład warunkowy $p(x|Y=k)$ jest rozkładem normalnym o gęstości $N(\\mu_k, \\Sigma_k)$, gdzie $\\mu_k = (\\mu_{1k}, \\ldots, \\mu_{pk})^T$ jest wektorem średnich cech dla klasy $k$, a $\\Sigma_k$ jest **diagonalną** macierzą kowariancji cech dla klasy $k$ (wymaga obliczenia $p$ parametrów $\\sigma_{jk}^2$ na głównej przekątnej, poza nią macierz jest zerowa).\n",
    "* Inna opcja dla zmiennej losowej ciągłej to zdefiniowanie rozkładu $p(x_j|Y=k)$ na podstawie histogramu lub estymatora jądrowego. \n",
    "* Jeśli cecha $X_j$ jest kategoryjna (ma skończony zbiór wartości), to prawdopoobieństwo $p(x_j|Y=k)$ możemy estymować na podstawie częstości występowania poszczególnych wartości cechy $X_j$ w klasie $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metody klasyfikacji - porównanie\n",
    "\n",
    "Ustalmy klasę $K$ jako punkt odniesienia i rozważmy logarytmy stosunków prawdopodobieństw:\n",
    "\n",
    "$$\n",
    "\\log\\frac{P(Y=k|x)}{P(Y=K|x)},\\qquad k=1,\\ldots,K.\n",
    "$$\n",
    "\n",
    "Obserwację $x$ przypisujemy do klasy $k$, dla której logarytm ten jest największy. W przypadku modelu LDA mamy\n",
    "\n",
    "\\begin{align*}\n",
    "\\log\\frac{P(Y=k|x)}{P(Y=K|x)} &= \\log\\frac{P(Y=k)}{P(Y=K)} + \\log\\frac{p(x|Y=k)}{p(x|Y=K)} \\\\\n",
    "&= \\log\\frac{P(Y=k)}{P(Y=K)} - \\frac{1}{2}(\\mu_k+\\mu_K)^T\\Sigma^{-1}(\\mu_k-\\mu_K) + x^T\\Sigma^{-1}(\\mu_k-\\mu_K)\\\\\n",
    "&= a_k +\\sum_{j=1}^p b_{kj}x_j,\n",
    "\\end{align*}\n",
    "\n",
    "gdzie $a_k$ i $b_{kj}$ są stałymi niezależnymi od $x$. Zatem logarytmy szans w modelu LDA są funkcjami liniowymi względem $x$ dokładnie tak samo jak w przypadku regresji logistycznej. Różnica między tymi modelami polega na tym, że w regresji logistycznej szukamy parametrów $a_k$ i $b_{kj}$ bezpośrednio, natomiast w modelu LDA obliczamy najpierw parametry $P(Y=k)$,\n",
    "$\\mu_k$ i $\\Sigma$ i dopiero na ich podstawie uzyskujemy parametry $a_k$ i $b_{kj}$. Zatem w modelu regresji logistycznej mamy mniej parametrów do dopasowania niż w modelu LDA. Ponadto w tym pierwszym przypadku nic nie zakładamy na temat postaci rozkładów warunkowych $p(x|Y=k)$ modelując od razu rozkład $P(Y=k|x)$. Jeśli spodziewamy się, że rozkłady warunkowe $p(x|Y=k)$ są rozkładami normalnymi, to model LDA może być lepszym wyborem niż regresja logistyczna. W przeciwnym przypadku wybieramy regresję logistyczną jako bardziej uniwersalny i mniej kosztowny obliczeniowo model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W modelu QDA mamy\n",
    "\n",
    "\\begin{align*}\n",
    "\\log\\frac{P(Y=k|x)}{P(Y=K|x)} &= a_k +\\sum_{j=1}^p b_{kj}x_j + \\sum_{j=1}^p\\sum_{l=1}^p c_{kjl}x_jx_l,\n",
    "\\end{align*}\n",
    "\n",
    "gdzie $a_k$, $b_{kj}$ i $c_{kjl}$ znów nie zależą od $x$ i są funkcjami parametrów modelu $P(Y=k)$, $\\mu_k$ i $\\Sigma_k$. Logarytmy szans w modelu QDA są funkcjami kwadratowymi względem $x$. \n",
    "\n",
    "W naiwnym klasyfikatorze Bayesa mamy\n",
    "\n",
    "\\begin{align*}\n",
    "\\log\\frac{P(Y=k|x)}{P(Y=K|x)} &= \\log\\frac{P(Y=k)}{P(Y=K)} + \\sum_{j=1}^p\\log\\frac{p(x_j|Y=k)}{p(x_j|Y=K)} \\\\\n",
    "&= a_k +\\sum_{j=1}^p g_{kj}(x_j),\n",
    "\\end{align*}\n",
    "\n",
    "gdzie $a_k=\\log(P(Y=k)/P(Y=K))$ i $g_{kj}(x_j)=\\log(p(x_j|Y=k)/p(x_j|Y=K))$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z powyższych przedstawień wynika:\n",
    "\n",
    "* QDA sprowadza się do LDA, gdy $c_{kjl}=0$ dla wszystkich $k$, $j$ i $l$. Dzieje się tak dokładnie wtedy, gdy wszystkie macierze kowariancji $\\Sigma_k$ są równe.\n",
    "* Kładąc $g_{kj}(x_j)=b_{kj}x_j$ w naiwnym klasyfikatorze Bayesa otrzymujemy model LDA z niezależnymi cechami (macierz kowariancji $\\Sigma$ jest diagonalna).\n",
    "* QDA nie sprowadza się do naiwnego klasyfikatora Bayesa, ani naiwny klasyfikator Bayesa nie sprowadza się do QDA. W modelu Naive Bayes cechy są warunkowo niezależne, ale mogą być modelowana dowolnymi rozkładami. Z kolei w modelu QDA cechy mogą być skorelowane, ponadto model uwzględnia interakcje między nimi."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
